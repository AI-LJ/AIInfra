<!--Copyright © ZOMI 适用于[License](https://github.com/chenzomi12/AIInfra)版权许可-->

# NV Blackwell 产品演进分析

本节内容主要介绍 NVIDIA 最新一代架构 BlackWell 的整体架构演进与相关产品形态。

图1中概述了在BlackWell架构之前NVIDIA GPU的所有架构演进的过程，涵盖了从2010年的Fermi到2022年的Hopper各个时期。其中详细列举了各架构的发布时间、核心组成、主要特点与优势、制造工艺（纳米制程），以及代表型号。我们可以发现NVIDIA GPU技术在过去十余年间的显著发展，包括在核心设计、性能提升及制造工艺上的持续进步。

![NVIDIA GPU的架构演进](./images/Evolution01.png)


作为NVIDIA最新架构， Blackwell架构产品家族拥有从基础芯片到大规模系统部署的完整生态。从一块芯片到一个模组，再到一个Compute Tray，再到整个机柜，这种方式衍生了非常多不同型号的产品，从而产生了众多新的命名方式与产品名词，接下来我们将通过详细介绍Blackwell架构产品家族来搞清楚其真正含义以及产品背后代表的哪些新的AI方向与新产品的演进方向。

![Blackwell架构产品家族](./images/Evolution02.png)

本节内容主要分为三部分：GPU产品介绍；HGX产品介绍；NVL产品介绍

## GPU产品介绍

下图展示了NVIDIA主要GPU产品及其关键技术规格的对比，涵盖了从Ampere架构的A100到最新Blackwell架构的B系列产品。

首先，我们可以从架构演进的角度来看。表格清晰地展示了NVIDIA从Ampere（代表产品A100）发展到Hopper（代表产品H100、H200、GH200），再到最新的Blackwell（代表产品B100、B200、Full B200、GB200）的路线图。这种迭代更新体现了NVIDIA在高性能计算和AI领域的持续技术投入。

其次，存储性能是衡量GPU能力的重要指标。在HBM大小和HBM带宽两行中，我们可以看到显著的提升。从A100的80GB HBM和2TB/s带宽，逐步提升到H200的141GB和4.8TB/s，而Blackwell架构的产品更是实现了飞跃，B100和B200拥有180GB/192GB HBM和8TB/s带宽，GB200更是达到了惊人的384GB HBM和16TB/s带宽。这表明NVIDIA正不断扩大显存容量和带宽，以满足日益增长的AI模型对数据吞吐量的需求。

再者，计算能力是GPU的核心竞争力。表格详细列出了多种浮点运算（FLOPs）和整数运算（OPS）的性能指标。从FP16到FP4，我们可以看到：

* FP16 FLOPs 从A100的312T提升到H100的1P，Blackwell系列更是达到1.75P至5P。
* INT8 OPS 也从624T飙升至2P甚至Blackwell的10P。
* FP8、FP6和FP4 是Blackwell架构引入或大幅增强的计算特性，Hopper架构虽支持FP8但Blackwell有更高性能，而FP6和FP4仅在Blackwell系列中支持，尤其FP4在GB200上达到了20P。

NVLink带宽的提升同样关键，它衡量了GPU之间以及GPU与CPU之间的数据传输速度。从A100的600GB/s到Hopper的900GB/s，再到Blackwell的1.8TB/s，GB200甚至达到3.6TB/s，这使得构建大规模多GPU系统和实现CPU-GPU协同工作变得更加高效。

最后，功耗（Powers）是性能提升伴随的考量，从A100的400W到Blackwell系列的700W至2700W（GB200），反映了为实现更高性能所需的能源消耗。

值得注意的是，在芯片设计中，“1 Die”（单晶粒）表示单个封装内仅含一个独立的硅晶粒，而“2 Die”（双晶粒）则意味着一个封装内实际包含两个独立的硅晶粒。这两个晶粒通常通过高速互联技术（如NVIDIA的NVLink-C2C）实现无缝连接，在逻辑上协同工作，形成一个更强大的整体。
在Blackwell架构问世之前，包括Ampere和Hopper在内的NVIDIA GPU架构均采用单晶粒（1 Die）设计。然而，Blackwell架构则引入了革命性的双晶粒（2 Die）构造。这一设计上的根本性转变使得一张Blackwell架构的B100芯片，在计算能力和资源上，实际上等同于传统意义上的两张GPU卡。
正因如此，在Blackwell架构的系统配置中，例如在一个计算托盘（Trays板）内，您会观察到它可能包含8张Ampere或Hopper架构的GPU芯片，但若配置Blackwell架构的B系列芯片，则仅需4张B系列芯片即可达到相同的总卡概念（即4张B芯片等同于8张单Die卡的算力），这充分体现了双晶粒设计在提升单芯片集成度和性能密度方面的显著优势。

![GPU产品](./images/Evolution03.png)


下图则表示B100芯片的概念图。

![B100](./images/Evolution04.png)


接下来，我们将详细分析FP16浮点运算能力在这三代GPU架构中的显著变化。

首先，从Ampere架构的A100到Hopper架构的H100，FP16算力实现了超过三倍的惊人增长（从312 TFLOPs提升至1 PFLOP）。值得注意的是，尽管性能大幅提升，但功耗仅从400瓦增加到700瓦。随后，从Hopper架构的H200到Blackwell架构的B200，FP16算力再次实现了超过两倍的提升（从1 PFLOP提升至2.25 PFLOP），而同期功耗仅增加了300瓦（从700瓦增至1000瓦）。这些数据清晰地揭示了芯片制程工艺进步所带来的两大核心优势：一是能够实现几何级的性能飞跃，二是能够尽可能地抑制功耗的同步大幅增长。这种高效的性能提升模式，正是半导体制造工艺持续演进的强大驱动力。

下面则引出一个问题：DGX B200是否适用于大模型推理任务呢？

![DGX B200](./images/Evolution05.png)

如上图所示，在训练场景下，DGX B200 相较于 DGX H100 展现出约三倍的性能提升；而在推理场景下，其性能提升更是高达15倍以上。这一数据强烈暗示 DGX B200 在大模型推理任务中具备显著优势。

接下来，我们将对DGX B200实现如此性能提升的内在机制进行深入分析：

首先，从硬件规格来看，DGX B200 相较于 DGX H100 在FP16浮点运算能力上提升了2.25倍。与此同时，其高带宽内存（HBM）容量显著增大，显存带宽也实现了2.23倍的提升，而NVLink的互联带宽更是直接翻倍。在这些核心硬件性能指标的全面跃升下，若假设模型算力利用率（MFU）能够达到50%，那么整体训练性能实现三倍左右的提升是完全符合硬件增益逻辑的。

然而，对于推理场景，我们认为DGX B200的适用性并非全然乐观，甚至在某些情况下，其所谓的巨大性能提升可能需要更审慎的评估。图中所示的“15倍推理性能提升”，是通过比较B200的FP4算力与H100的FP8算力得出的。这种跨精度等级的比较并不完全对等，因为FP4相较于FP8会引入更高的量化误差，可能不适用于所有对精度要求严苛的推理任务。此外，在推理场景下，相较于训练，对性价比的考量往往更为关键。尽管B200在峰值性能上表现卓越，但其高昂的成本和在实际推理部署中是否能充分发挥FP4性能（例如，并非所有模型都支持或需要FP4推理，或者FP4推理需要额外的模型量化工作），这些因素都可能影响其在推理场景下的实际效益和普及程度。


## HGX产品介绍

在介绍HGX产品之前，我们先搞清楚几个特殊的产品名词。

* HGX（Hyperscale GPU eXchange） 是一个 GPU 模块化平台，主要提供给 OEM 厂商（如 Inspur、Dell、Supermicro） 来组装自己的高性能服务器。其核心是一个 GPU 托盘（Tray）或主板模块，集成了多张 GPU 及其互连网络（如 NVLink/NVSwitch）。其中不包含 CPU、存储等部件，需 OEM 厂商整合成完整服务器（例如 Supermicro SYS 系列）。总结来说，HGX 是“硬件模块 + 高速互联”，用于构建高密度、模块化、可扩展的 GPU 集群。
* DGX（Deep GPU Xceleration）是 NVIDIA 提供的一整套高性能服务器系统，集成了GPU、CPU、内存、存储、网络等全套组件，即开即用，专为训练和推理大模型设计。这类产品由 NVIDIA 亲自设计、制造和销售。


![alt text](./images/Evolution06.png)

这张表格详尽地展示了NVIDIA不同代次HGX平台产品的关键技术规格对比，涵盖了从Ampere架构的HGX A100到最新Blackwell架构的HGX B100和HGX B200。

表格首先列出了产品名称，清晰指明了每款HGX配置所搭载的GPU类型和数量（例如，HGX A100通常配置8颗A100 SXM GPU）。随后，它按照架构进行了分类：Ampere、Hopper和Blackwell，体现了NVIDIA产品线的迭代演进。

核心指标方面，表格详细对比了以下几个关键性能参数：

HBM大小和HBM带宽：这些数据反映了系统总体的显存容量和数据吞吐能力。可以看到，从HGX A100的640GB HBM和2TB/s带宽，逐步提升到HGX H100的1.1TB HBM和3.35TB/s带宽，再到HGX B200惊人的1.5TB HBM和8TB/s带宽，显存容量和带宽都呈现出显著的增长趋势，以满足日益庞大的AI模型对显存和数据传输的需求。

浮点和整数运算能力 (FLOPs/OPS)：表格列出了不同精度（FP16、INT8、FP8、FP6、FP4）的理论峰值算力。

FP16 (Half-precision floating-point) 算力从HGX A100的312T FLOPs提升到HGX H100的1 PFLOPs，再到HGX B200的2.25 PFLOPs，显示了AI训练能力的大幅提升。
INT8 (8-bit integer) 算力同样从624T OPS跃升至4.5P OPS，表明在推理任务中的效率显著提高。
FP8、FP6和FP4 是更低精度的浮点运算，其中FP8在Hopper架构中开始支持，而FP6和FP4则主要在Blackwell架构中出现并显示出极高的性能（例如HGX B200的FP4达到9P FLOPs），这对于大模型推理和进一步压缩计算量至关重要。
互联带宽：

GPU-GPU带宽：衡量了HGX模块内部GPU之间的数据传输速度，从A100的600 GB/s提升到H100的900 GB/s，再到Blackwell的1.8 TB/s，确保了多GPU协同计算的高效性。
NVLink带宽：这是GPU之间，乃至HGX模块之间的高速互联总带宽，从HGX A100的4.8 TB/s到HGX H100的7.2 TB/s，再到HGX B200的14.4 TB/s，其翻倍的增长对于构建更大规模的AI集群至关重要。
网络带宽：表格还包括了外部网络连接的能力，如Ethernet带宽和IB（InfiniBand）带宽。这些指标从HGX A100的200 Gb/s以太网和8 x 200 Gb/s IB，提升到HGX B200的2 x 400 Gb/s以太网和8 x 400 Gb/s IB，体现了系统对外数据传输能力的显著增强，这对于分布式训练和集群扩展至关重要。

功耗 (Power)：分GPUs Power（GPU核心功耗）和总Power（整个HGX服务器的功耗）。可以看到，随着性能的提升，功耗也相应增加，从HGX A100的6.5kw总功耗，跃升到HGX B200的14.3kw，这反映了高性能计算对电力和散热基础设施的更高要求。

网络产品：展示了各代HGX平台所搭载的网络适配器或DPU（数据处理器），如ConnectX系列网卡和BlueField-3 DPU，它们提供了高性能的网络连接和卸载能力。

总体而言，这张表格清晰地描绘了NVIDIA HGX平台在每个架构迭代中，从GPU核心性能、显存、内部互联到外部网络连接能力的全面且显著的提升，展示了NVIDIA为应对日益增长的AI和HPC工作负载所做的持续努力和技术突破。


通过深入分析NVIDIA HGX平台的产品规格演变，我们可以清晰地描绘出其技术迭代的轨迹。首先，从Ampere架构的HGX A100迈向Hopper架构的HGX H100/H200时，FP16浮点运算能力实现了约3.2倍的飞跃，与此同时，总功耗的增幅却控制在2倍以内，这充分彰显了新一代纳米制程工艺在提升计算效率和优化能耗方面的卓越贡献。紧接着，从Hopper架构的HGX H100/H200过渡到Blackwell架构的HGX B100/B200，FP16算力再次取得了约2倍的显著增长，而令人惊喜的是，系统总功耗却基本保持不变，这主要归功于Blackwell架构引入的创新性双晶粒（2 Die）设计，有效提升了封装内的计算密度。然而，在外部网络连接方面，尽管GPU核心性能实现了跨越式发展，但Blackwell架构的InfiniBand（IB）带宽仍维持在8x400Gb/s，其升级速度相较于GPU算力的爆发式增长显得相对滞缓，这可能预示着未来超大规模AI集群在节点间通信方面可能面临新的挑战。


## NVL & SuperPod 产品介绍

这张表格全面介绍了NVIDIA用于构建大规模AI计算集群的NVL（NVLink）模块和SuperPOD系统，涵盖了Hopper架构（NVL 32, GH200 SuperPod）和Blackwell架构（NVL72, GB200 SuperPod）的主要产品及其关键性能指标。

**首先，从架构和产品构成来看：**
表格清晰地将产品分为Hopper和Blackwell两大架构。Hopper架构的产品包括：
* **NVL 32**：这是一个基础模块，包含32颗GH200 GPU。
* **GH200 SuperPod**：由256颗GH200 GPU组成，是大型Hopper集群的核心。

Blackwell架构的产品则包括：
* **NVL72**：这是Blackwell架构下的基础模块，内含36颗GB200 GPU。值得注意的是，GB200 GPU本身是双晶粒（2 Die）设计，且集成了Grace CPU，因此“36 x GB200”实际上代表了72颗B200 GPU和36颗Grace CPU，这在逻辑上等同于72个GPU的概念。
* **GB200 SuperPod**：由288颗GB200 GPU构成，是Blackwell架构下目前公布的超大规模AI系统，其计算密度和总算力达到了前所未有的水平。

**其次，在核心性能指标上，表格展示了代际间的显著提升：**

1.  **内存与带宽**：HBM总大小从GH200 SuperPod的24.5TB（LPDDR5X为123TB）大幅跃升至GB200 SuperPod的110TB，显存带宽也从Hopper的4.8TB/s提升到Blackwell的8TB/s。这表明新架构为超大规模AI模型提供了更广阔的内存空间和更快的数据存取速度。

2.  **计算能力 (FLOPs/OPS)**：
    * **FP16** 算力从GH200 SuperPod的256 PFLOPs，飙升至GB200 SuperPod的1440 PFLOPs（即1.44 EFLOPS），实现了超过5倍的巨大飞跃，展现了Blackwell架构在AI训练任务中的强大潜力。
    * **INT8和FP8** 算力也呈现相似的倍数增长，对于推理任务至关重要。
    * 值得注意的是，Blackwell架构引入并大幅提升了**FP6和FP4**的计算能力，其中GB200 SuperPod的FP4算力高达5760 PFLOPs（5.76 EFLOPS），这预示着在低精度推理和更高效的AI应用方面将有突破性的进展。

3.  **内部互联带宽**：
    * **GPU-GPU带宽** 从Hopper架构的0.9TB/s提升到Blackwell架构的1.8TB/s，翻了一番。
    * **NVLink带宽** 的提升尤为显著，从GH200 SuperPod的230TB/s，跃升至GB200 SuperPod的1PB/s（1000TB/s），这对于实现GPU集群内的高效数据同步和模型并行训练至关重要，确保了超大规模集群的线性扩展能力。NVLink Switch也从Gen3 64 Port升级到Gen4 72 Port，提升了交换能力。

4.  **外部网络带宽**：
    * **Ethernet带宽** 从256 x 200Gb/s提升到576 x 400Gb/s，总带宽大幅增加。
    * **IB带宽** 从256 x 400Gb/s提升到576 x 800Gb/s，这表明NVIDIA正在为构建更大、更复杂的分布式AI训练和推理集群提供更强大的外部通信能力，以减少网络瓶颈。

5.  **总功耗**：随着性能的急剧提升，功耗也相应增加。GH200 SuperPod的总功耗为256kW，而GB200 SuperPod的总功耗高达777.6kW，这反映了这些超级计算系统对电力和散热基础设施的极致要求。

6.  **网络产品**：Hopper架构的HGX使用ConnectX-7 NIC，而Blackwell架构则升级到ConnectX-8 NIC，提供了更高的网络性能和更先进的网络卸载功能。

综上所述，这张表格系统地展示了NVIDIA在构建大规模AI超算平台上的技术演进和领先优势。通过持续提升GPU核心性能、扩展内存带宽、增强GPU内部和集群间互联能力，NVIDIA的NVL模块和SuperPOD系统为推动AI大模型训练和推理的边界提供了无与伦比的计算基础。


NVIDIA大规模AI计算平台从Hopper到Blackwell架构的升级，带来了核心计算能力的巨大飞跃。

首先，在基础模块层面，从NVL32（32颗GH200 GPU，32 PFLOPs FP16算力）到NVL72（72颗GB200 GPU，180 PFLOPs FP16算力），FP16性能实现了近六倍的提升，这得益于GPU数量的增加和Blackwell架构单卡性能的显著增强。

其次，在构建超大规模SuperPOD系统时，这种性能倍增效应得以延续。GH200 SuperPod（256颗GH200 GPU，256 PFLOPs FP16算力）升级至GB200 SuperPod（576颗GB200 GPU，1440 PFLOPs FP16算力），FP16性能同样实现了接近六倍的增长，确保了集群扩展时的线性收益。

最后，Blackwell架构的NVL72和GB200 SuperPod均采用了更先进的ConnectX-8 InfiniBand网卡，提供800Gb/s带宽，保障了大规模集群内部和集群间的高效数据传输，为训练和部署巨型AI模型提供了坚实的基础。

## 总结与思考

整个B系列的发展对于国内芯片与计算服务器的发展有非常多的启发。

有一个趋势，从之前最初堆芯片的方式慢慢形成整一套大规模集群解决方案，这是很明显的发展趋势。不断的扩大规模。

NVIDIA B系列（Blackwell架构）的蓬勃发展，无疑为国内芯片与计算服务器产业的未来指明了极具价值的启发方向。

其中一个最为显著且不可逆转的趋势是，计算架构正从最初相对简单的**“芯片堆叠模式”，逐步演进为提供“整套大规模集群解决方案”**的复杂生态。NVIDIA B系列（如GB200 SuperPOD）的出现，不再仅仅是推出更强大的GPU芯片，而是着眼于如何将数以百计、乃至千计的高性能GPU，通过超高带宽的NVLink互联、先进的网络架构（ConnectX-8 InfiniBand）、以及精密的软件协调层，无缝地整合为一个具备极致扩展能力和计算效率的统一体。这不仅仅是硬件性能的线性累加，更是通过系统级优化，将计算、存储、网络融为一体，以应对万亿参数级别大模型训练和推理的严峻挑战。

这种**“不断扩大规模，并提供完整端到端解决方案”的发展模式，对于国内芯片设计企业和计算服务器制造商而言，具有深远的战略意义。它提示我们，未来的竞争将不仅仅局限于单个芯片的性能指标，更在于构建能够高效协同、易于部署和管理的全栈式AI计算基础设施**。这意味着，除了在芯片设计上持续追赶国际先进水平，国内厂商还需在高速互联技术（如类NVLink）、大规模集群管理软件、高性能散热与供电系统，乃至定制化的数据中心部署方案等多个维度进行深度布局和创新，才能在未来的AI算力竞赛中占据一席之地。简而言之，竞争的焦点已从“造好芯”拓展到“搭好台”，再到“唱好戏”，最终形成一个无缝连接、高度优化的完整计算生态。